# live_streaming_lists_extension

유튜브, 치지직, 트위치, 아프리카TV같은 스트리밍 사이트에서 현재 인기있는 방송들을 모아서 크롬탭 하나에 리스트로 보여주는 프로젝트입니다.  
기회가 된다면 배포까지 할 생각이 있기 때문에 (크롬 확장프로그램으로 배포할 예정) 7월 3일까지 완성하지 못해도,  
시간이 얼마나 걸리던 아마 저는 끝까지 프로젝트를 진행할 것 같습니다.  

> 1달이라는 시간때문에, 프로젝트 성능 개선보다는,  **프로젝트 기능 구현**에 초점을 맞추는것을 목적으로 하고 있습니다.
> 기간이 넘어가더라도 추후, 협의를 통해 프로젝트를 이어나갈 예정입니다.

개발 시 참고할 확장프로그램은 [hackertab.dev](https://hackertab.dev/)입니다. 한번 사용해 보시는게 좋을 것 같아요

## 유튜브 :
[Youtube data v3 api](https://developers.google.com/youtube/v3/docs/search/list?hl=ko) 사용하면 됩니다.
`Search`를 사용해서 `live`인 것들만 가져오면 되고, Chat GPT한테 물어보니까 API를 호출해서 인기있는 방송만 가져오는것은 한계가 있다고 합니다.
일단 API를 호출해서 한국 지역의 방송들을 가져오도록 합시다.

또한, 말왕TV같은 채널이 라이브 방송을 할 때, 유튜브 실시간 페이지에 노출이 되지 않습니다.
이러한 경우가 다수 존재할 것이므로, Youtube API를 사용하여 특정 채널을 여러개 지정해서 Channel ID를 수집해 미리 저장해두고, 위에서 사용한 방식처럼 
해당 API를 호출할 때 Channel ID를 집어넣어서 라이브 스트리밍 데이터를 가져오는 방식을 수행하면 됩니다.

만약 데이터 수집 시 수동적으로 선택해서 API 호출한 결과와 전체적으로 수집한 API 데이터에 중복이 발생한다면 하나만 선택해서 반영하는것으로 해야합니다.

**유튜브 실시간 스트리밍 사이트는 정확한 수의 시청자수를 반영하기 위해 대략 2초정도의 간격으로 업데이트 된다고 합니다.**
API 호출에 한계가 존재하기 때문에 2초마다 데이터베이스에 반영하는건 무리가 있을것입니다. 
따라서 처음에는 5분간격으로 했다가, 시스템 성능을 평가한 후에 점점 주기를 줄여나가면 됩니다.

---

만약 API로 수집한 데이터에 한국어가 아닌 영문 방송이나 인도, 아랍 언어가 포함되어 있다면 ChatGPT prompt 기능을 통해 수집한 데이터를 필터링해주면 될 것 같습니다.
또한 ChatGPT를 사용해서 유튜브에서 제공하지 않고있는 실시간방송의 분야 태그도 분류할 수 있다고 합니다.

## 치지직 :
공식적으로 지원하는 API가 없어서 직접 스크래핑을 해야하고, 초기 스크래핑 테스트코드는 완성되었습니다.
** 치지직은 새로고침 계속 해봤는데 1분 주기로 시청자수가 갱신되는 것 같습니다.** 마찬가지로 1분마다 스크래핑하기에는, IP block같은 문제가 존재하기 때문에 5분정도 널널하게 잡아두고 점점 줄여나가면 될 것 같습니다. **

## 아프리카 TV :
치지직과 마찬가지로 스크래핑을 통해 데이터를 가져오면 됩니다. 초기 스크래핑 테스트코드는 완성되었습니다.

## 스크래핑 주기
실시간 방송의 시청자 수는 계속 변하기 때문에 아마 5분 주기로 계속 API 호출 또는 스크래핑을 해서 Google Cloud Database에 데이터를 저장하고, 썸네일을 링크로 저장하는 대신 AWS S3나 Google Cloud Storage를 사용해서 썸네일 이미지를 저장해주는 방식을 사용해서, 클라이언트가 데이터를 가져올 때 더 빠르게 가져올 수 있도록 합시다.

## CDN에서 이미지를 배포하는게 더 효율적인 이유

[링크](https://github.com/Scanf-s/live_streaming_lists/blob/main/whyCDN.md)

## Kafka에 대해서
[링크](https://github.com/Scanf-s/live_streaming_lists/blob/main/what_is_kafka.md)

지금 상황에서는 데이터 수집해서 바로 DB나 스토리지에 데이터를 저장하는것이 더 빠를수 있는데, 나중에 스크래핑 주기를 5분에서 점점 줄여나가게 된다면
1분 또는 몇 초 주기로 얻어오는 데이터에 대해 실시간 데이터 처리를 빠르게 DB에 저장해야하기 때문에 Kafka를 사용하는것이 더 좋을 것 같습니다.  
(저도 아직 Kafka에 대해서 개념만 조금 알고있어서 잘 알지는 못합니다.)

프론트에서 데이터를 가져올때는 Kafka를 거치는것이 아니라, 직접 DRF로 구성된 백엔드에서 API호출을 통해 데이터를 가져오면 됩니다.

## 개발 환경
- Ubuntu
- MacOS
(둘다 UNIX 기반이라 아무거나 사용하시면 됩니다.)

## 파이썬 버전 관리 및 가상 환경 툴
- pyenv
- poetry

## 사용 기술
- Python
- Django
- Selenium, BeautifulSoup4
- PostgreSQL
- (만약 프론트가 있다면) React.js
- 그 이외...
